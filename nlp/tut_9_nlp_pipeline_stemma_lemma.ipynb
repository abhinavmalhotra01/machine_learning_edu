{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# pipeline = bunch of components\n",
        "# text => tokenization => pipeline => doc"
      ],
      "metadata": {
        "id": "gFysOLk1RKa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "kqUZWx8PRKll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "doc = nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day\")\n",
        "\n",
        "for token in doc :\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZnRvmbvwjc2",
        "outputId": "bfe11145-00d6-42d4-9369-de146d1d682b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Captain\n",
            "america\n",
            "ate\n",
            "100\n",
            "$\n",
            "of\n",
            "samosa\n",
            ".\n",
            "Then\n",
            "he\n",
            "said\n",
            "I\n",
            "can\n",
            "do\n",
            "this\n",
            "all\n",
            "day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jT1j4nfGwzKq",
        "outputId": "2e942727-f76b-4d86-d2f3-cf0f31427173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for each lang , we can use/download trained pipeline for that lang\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHhXv8Klw1fK",
        "outputId": "17388d9f-3594-4ba3-e675-cae1a1f997ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-04 17:16:52.140204: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.5.0) (3.5.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\") # upr blank tha spacy.load , yaha trained pipeline daali h"
      ],
      "metadata": {
        "id": "SOMi2URlxH-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWDxT7hGxPEP",
        "outputId": "74432bad-c8fd-4c5b-9160-6389c83f870b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRFbHLX5xago",
        "outputId": "a80eebd4-e1ea-4ff0-f73e-b557ed83a296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7f2f9a973e80>),\n",
              " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7f2f9a973fa0>),\n",
              " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7f2f9a9bbd80>),\n",
              " ('attribute_ruler',\n",
              "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7f2f9aa78ac0>),\n",
              " ('lemmatizer',\n",
              "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7f2f9a7c2200>),\n",
              " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7f2f9a9bbe60>)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day\")\n",
        "\n",
        "for token in doc :\n",
        "  print(token, \" | \" , token.pos_ , \" | \" , token.lemma_)\n",
        "# pos -> part of speech -> each word has a grammer identity -> example -> verb , noun , pronoun\n",
        "# lemma -> base word of every word -> example -> eat is base word for eating"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfGXP6jaxdAk",
        "outputId": "bd0b6139-f091-4859-ba42-622ad0ca7aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Captain  |  PROPN  |  Captain\n",
            "america  |  PROPN  |  america\n",
            "ate  |  VERB  |  eat\n",
            "100  |  NUM  |  100\n",
            "$  |  NUM  |  $\n",
            "of  |  ADP  |  of\n",
            "samosa  |  PROPN  |  samosa\n",
            ".  |  PUNCT  |  .\n",
            "Then  |  ADV  |  then\n",
            "he  |  PRON  |  he\n",
            "said  |  VERB  |  say\n",
            "I  |  PRON  |  I\n",
            "can  |  AUX  |  can\n",
            "do  |  VERB  |  do\n",
            "this  |  PRON  |  this\n",
            "all  |  DET  |  all\n",
            "day  |  NOUN  |  day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
        "\n",
        "# ent -> entities \n",
        "for ent in doc.ents :\n",
        "  print(ent.text, \" | \" , ent.label_ , \" | \", spacy.explain(ent.label_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gv44p49kxzWm",
        "outputId": "fdd406ab-8197-409c-aa62-4a32d6cfca54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
            "$45 billion  |  MONEY  |  Monetary values, including unit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy \n",
        "\n",
        "displacy.render(doc,style=\"ent\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "QlatWm9ayN3Q",
        "outputId": "3d1b8af3-cf85-4b4a-c2a5-81cca3a4fb40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\\n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\\n    Tesla Inc\\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\\n</mark>\\n is going to acquire twitter for \\n<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\\n    $45 billion\\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\\n</mark>\\n</div>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can also add custom pipe\n",
        "source_nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp=spacy.blank(\"en\")\n",
        "nlp.add_pipe(\"ner\",source=source_nlp)\n",
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFeNyodZzwjD",
        "outputId": "d09b074f-d884-42b5-f646-46448627a8ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ner']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Stemming and Lemmatization***"
      ],
      "metadata": {
        "id": "fnTyIlxR0MDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming -> just cutting down the suffixed and getting base word \n",
        "# example -> talking => talk ; eating => eat\n",
        "# dumb rules \n",
        "\n",
        "# lemmatization -> use knowledge of language , derive a base word \n",
        "\n",
        "\n",
        "# spacy -> no stemming , only lemma\n",
        "# nltk -> both"
      ],
      "metadata": {
        "id": "A6akfTh40YiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "ZC9fN21L01oZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer \n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "fWstu1es04Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"eating\" , \"eats\" , \"eat\" , \"ate\" , \"adjustable\" , \"rating\" , \"ability\" , \"meeting\"]\n",
        "for word in words :\n",
        "  print(word , \" | \" , stemmer.stem(word) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4--QCDM0-kn",
        "outputId": "bdfeac75-4796-4f81-c5ac-2c0223f35958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating  |  eat\n",
            "eats  |  eat\n",
            "eat  |  eat\n",
            "ate  |  ate\n",
            "adjustable  |  adjust\n",
            "rating  |  rate\n",
            "ability  |  abil\n",
            "meeting  |  meet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming -> ate : ate , ability : abil ; dumb library"
      ],
      "metadata": {
        "id": "KKe2ZwVu1SF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lemma ->\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"eating eats eat ate adjustable rating ability meeting\")\n",
        "for token in doc :\n",
        "  print(token , \" | \" , token.lemma_) # token.lemma -> gives hashword / kindof ascii of lemma_ "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r40lTD8L1aSG",
        "outputId": "bd3fe030-a4ad-40e9-cf90-8457e4df071c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating  |  eat\n",
            "eats  |  eat\n",
            "eat  |  eat\n",
            "ate  |  eat\n",
            "adjustable  |  adjustable\n",
            "rating  |  rating\n",
            "ability  |  ability\n",
            "meeting  |  meeting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# better and smart ; but takes time "
      ],
      "metadata": {
        "id": "HmqO9AeN1011"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# attribute ruler component in pipeline -> customize pipeline\n",
        "\n",
        "ar = nlp.get_pipe('attribute_ruler')\n",
        "ar.add([[{\"TEXT\":\"Bro\"}],[{\"TEXT\" : \"Brah\"}]],{\"LEMMA\":\"Brother\"}) # customize\n",
        "doc = nlp(\"Bro , u wanna go ? Brah , dont say no , i am done\")\n",
        "\n",
        "for token in doc : \n",
        "  print(token.text , \" | \" ,token.lemma_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ws_uC-U-2C0O",
        "outputId": "d96cbeef-35aa-451f-912e-3f1e148589c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bro  |  Brother\n",
            ",  |  ,\n",
            "u  |  u\n",
            "wanna  |  wanna\n",
            "go  |  go\n",
            "?  |  ?\n",
            "Brah  |  Brother\n",
            ",  |  ,\n",
            "do  |  do\n",
            "nt  |  not\n",
            "say  |  say\n",
            "no  |  no\n",
            ",  |  ,\n",
            "i  |  I\n",
            "am  |  be\n",
            "done  |  do\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Smol exercise"
      ],
      "metadata": {
        "id": "BFqrLrBx4bKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = []\n",
        "lst_words = ['running', 'painting', 'walking', 'dressing', 'likely', 'children', 'whom', 'good', 'ate', 'fishing']\n",
        "for word in lst_words :\n",
        "  output.append(stemmer.stem(word))\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vv2IFF4Z3bJu",
        "outputId": "b417fdf9-66e2-485e-c5f3-82fb8c617013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'paint', 'walk', 'dress', 'like', 'children', 'whom', 'good', 'ate', 'fish']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = []\n",
        "doc = nlp(\"running painting walking dressing likely children who good ate fishing\")\n",
        "for token in doc :\n",
        "  output.append(token.lemma_)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTz0G65e35FX",
        "outputId": "89d12ba5-5961-49a4-f3e6-821276d70ce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'paint', 'walk', 'dress', 'likely', 'child', 'who', 'good', 'eat', 'fishing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c9G5cQbC4GqX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}